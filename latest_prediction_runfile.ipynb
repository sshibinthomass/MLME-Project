{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22d55c-883e-4bb7-b741-76d1886d27fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  LAG = 10,  horizon = 5\n",
      "\n",
      "âš™  Processing file_20726 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_49595 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_550 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_63816 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_96991 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š  Average error & coverage (open vs closed)\n",
      "\n",
      " Var  Open MAE  Closed MAE  Open MSE  Closed MSE  Open COV %  Closed COV %\n",
      "T_PM   0.16463     0.38134   0.06438     0.48875    99.65622      97.60406\n",
      "   c   0.00034     0.00074   0.00000     0.00000    94.56016      94.09137\n",
      " d10   0.00010     0.00010   0.00000     0.00000    57.59353      56.36548\n",
      " d50   0.00011     0.00011   0.00000     0.00000    51.10212      50.17259\n",
      " d90   0.00008     0.00008   0.00000     0.00000    46.41052      45.21827\n",
      "T_TM   0.17469     0.40536   0.07187     0.55408    99.65622      97.52284\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "\n",
    "Generates open-loop and closed-loop forecasts for the SFC data, with calibrated\n",
    "CQR prediction bands + full metrics & plots.\n",
    "\n",
    "Mapping to Fig S5 (page 65) as per info given:\n",
    "    â€¢ \"prediction model\" in the paper  â†’  **closed-loop** here\n",
    "    â€¢ \"simulation model\"               â†’  **open-loop** here\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€  Imports  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# â”€â”€  Paths & runtime constants  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TEST_DIR   = Path(r\"C:/Users/nishi/Desktop/MLME Project/dta\")\n",
    "MODEL_ROOT = Path(r\"C:/Users/nishi/Desktop/MLME Project/model_5files\")\n",
    "OUT_DIR    = MODEL_ROOT/'Results_CQR_5files'; OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ----------  Load metadata to stay in sync with training  -----------------\n",
    "\n",
    "# --- Unit configuration ---------------------------------------------------\n",
    "#USE_MICRONS = False        # True  âœ internally work in Âµm  \n",
    "PSD_COLS    = ('d10', 'd50', 'd90')\n",
    "\n",
    "\n",
    "meta         = json.loads((MODEL_ROOT/'metadata.json').read_text())\n",
    "STATE_COLS   = meta['state_cols']\n",
    "EXOG_COLS    = meta['exog_cols']\n",
    "LAG          = meta['lag']\n",
    "CLUST_COLS   = STATE_COLS + EXOG_COLS\n",
    "HORIZON      = 5         # closed-loop rollout length  (= Fig S5 horizon)\n",
    "\n",
    "print(f\"[INFO]  LAG = {LAG},  horizon = {HORIZON}\")\n",
    "\n",
    "# â”€â”€  Pre-processing helpers (same as training)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def read_txt(p): return pd.read_csv(p, sep='\\t', engine='python'\n",
    "                                   ).apply(pd.to_numeric, errors='coerce')\n",
    "def clean_df(df):\n",
    "    df = df.dropna(subset=CLUST_COLS)\n",
    "    df = df[(df.T_PM.between(250,400)) & (df.T_TM.between(250,400))\n",
    "            & (df.d10>0)&(df.d50>0)&(df.d90>0)\n",
    "            & (df.mf_PM>=0)&(df.mf_TM>=0)&(df.Q_g>=0)]\n",
    "    return df.reset_index(drop=True)\n",
    "#def harmonise_units(df):\n",
    "    \"\"\"\n",
    "    Make sure d10 / d50 / d90 are in *micrometres*.\n",
    "\n",
    "    Rule:\n",
    "        â€¢ If the median of a column is smaller than 1 Ã— 10â»Â² (i.e. < 1 cm)\n",
    "          the data must already be in metres  âœ  multiply by 1 Ã— 10â¶.\n",
    "        â€¢ Otherwise assume it is already Âµm and leave unchanged.\n",
    "\n",
    "    Works row-wise, so mixed units inside the same file are also fixed.\n",
    "    \"\"\"\n",
    "    if not USE_MICRONS:\n",
    "        return df    # fall-back for future experiments\n",
    "\n",
    "    for col in PSD_COLS:\n",
    "        median = df[col].median(skipna=True)\n",
    "        if median < 1e-2:         # < 1 cm  â‡’ data were metres\n",
    "            df[col] *= 1e6        # m â†’ Âµm\n",
    "    return df\n",
    "def to_metres(df):\n",
    "    \"\"\"\n",
    "    Ensure d10 / d50 / d90 are in metres, regardless of the fileâ€™s unit.\n",
    "\n",
    "    Heuristic:\n",
    "        â€¢ If the column median is > 0.01  (i.e. larger than 1 cm)\n",
    "          the numbers must be Âµm  â†’ divide by 1 Ã— 10â¶.\n",
    "        â€¢ Otherwise they are already metres â†’ leave untouched.\n",
    "    \"\"\"\n",
    "    for col in PSD_COLS:\n",
    "        if df[col].median(skipna=True) > 1e-2:   # > 1 cm â‡’ Âµm\n",
    "            df[col] /= 1e6                       # Âµm â†’ m\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(path: Path) -> pd.DataFrame:\n",
    "    df = clean_df(read_txt(path))\n",
    "    df = to_metres(df)        # <<< make sure we are in metres\n",
    "    return df\n",
    "\n",
    "\n",
    "# â”€â”€  Cluster artefacts  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sc_feat = pickle.loads((MODEL_ROOT/'feature_scaler.pkl').read_bytes())\n",
    "kmeans  = pickle.loads((MODEL_ROOT/'kmeans_model.pkl' ).read_bytes())\n",
    "\n",
    "def file_signature(df):\n",
    "    \"\"\"Return same feature vector used during training clustering.\"\"\"\n",
    "    arr = df[CLUST_COLS].values\n",
    "    return np.concatenate([arr.mean(0), arr.std(0), arr.min(0), arr.max(0)]).reshape(1,-1)\n",
    "\n",
    "def detect_cluster(df) -> int:\n",
    "    return int(kmeans.predict(sc_feat.transform(file_signature(df)))[0])\n",
    "\n",
    "# â”€â”€  Build lag matrix (newest-to-oldest)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_lagged(df, lag=LAG):\n",
    "    rows = []\n",
    "    for i in range(lag, len(df)-1):          # need y_{t+1} for target\n",
    "        row = []\n",
    "        for l in range(0, lag+1):            # 0 â€¦ LAG\n",
    "            idx = i - l\n",
    "            row.extend(df[CLUST_COLS].iloc[idx].values)\n",
    "        rows.append(row)\n",
    "    return np.asarray(rows, np.float32)\n",
    "\n",
    "# â”€â”€  Load per-cluster artefacts  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_cluster(cid):\n",
    "    scX = pickle.loads((MODEL_ROOT/f'narx/scaler_X_{cid}.pkl').read_bytes())\n",
    "    scY = pickle.loads((MODEL_ROOT/f'narx/scaler_Y_{cid}.pkl').read_bytes())\n",
    "    narx = tf.keras.models.load_model(MODEL_ROOT/f'narx/cluster_{cid}',\n",
    "                                      compile=False)\n",
    "    return scX, scY, narx\n",
    "\n",
    "# â”€â”€  Closed-loop rollout  (scaled space)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def rollout(model, lag_scaled, horizon, scX, scY, exog_future_raw):\n",
    "    \"\"\"\n",
    "    Predict horizon-step ahead sequence *closed-loop*:\n",
    "        at each step feed back the *predicted* (scaled) state,\n",
    "        plus the *true* exogenous input for that step.\n",
    "    \"\"\"\n",
    "    x      = lag_scaled.copy()              # shape = (input_dim,)\n",
    "    preds  = []\n",
    "    n_y    = len(STATE_COLS)\n",
    "    n_u    = len(EXOG_COLS)\n",
    "    stride = n_y + n_u                      # one (y+u) block per time step\n",
    "\n",
    "    # handy slices\n",
    "    y_slice  = slice(0, n_y)                # first part of each block\n",
    "    u_slice  = slice(n_y, n_y+n_u)          # second part of each block\n",
    "\n",
    "    for k in range(horizon):\n",
    "        y_scaled  = model.predict(x[None], verbose=0)[0]     # (n_y,)\n",
    "        y_raw     = scY.inverse_transform(y_scaled[None])[0]\n",
    "        preds.append(y_raw)\n",
    "\n",
    "        # --- push history backwards (newest first layout) ---\n",
    "        x[stride:] = x[:-stride] * 1.0      # shift older blocks\n",
    "        x[y_slice] = y_scaled               # newest state (pred)\n",
    "        # scale & insert exogenous truth for step k\n",
    "        mu_u   = scX.mean_[u_slice]\n",
    "        sig_u  = scX.scale_[u_slice]\n",
    "        x[u_slice] = (exog_future_raw[k] - mu_u) / sig_u\n",
    "\n",
    "    return np.asarray(preds)\n",
    "\n",
    "def predict_closed(df, scX, scY, narx, horizon=HORIZON):\n",
    "    \"\"\"\n",
    "    Rolling closed-loop prediction with stride = 1.\n",
    "    Returns\n",
    "        df_out   â€“ ground-truth rows matching predictions\n",
    "        Xs_all   â€“ scaled lag vectors   (for QR nets)\n",
    "        Yh_all   â€“ raw predictions      (shape rows Ã— |STATE|)\n",
    "    \"\"\"\n",
    "    total   = len(df) - 1 - LAG\n",
    "    usable  = total - horizon + 1\n",
    "    Xs_all, Yh_all = [], []\n",
    "\n",
    "    for t0 in trange(usable, desc=\"closed\", leave=False):\n",
    "        # lag vector (newest-first)\n",
    "        row  = []\n",
    "        for l in range(0, LAG+1):\n",
    "            idx = t0 + LAG - l\n",
    "            row.extend(df[CLUST_COLS].iloc[idx].values)\n",
    "        lag_raw  = np.asarray(row, np.float32)\n",
    "        lag_s    = scX.transform(lag_raw[None])[0]\n",
    "        exog_f   = df[EXOG_COLS].iloc[t0+1 : t0+1+horizon].values\n",
    "        y_seq    = rollout(narx, lag_s, horizon, scX, scY, exog_f)\n",
    "        Xs_all.append(lag_s)\n",
    "        Yh_all.append(y_seq[-1])            # only last step (t+h)\n",
    "\n",
    "    df_out = df.iloc[LAG+horizon : LAG+horizon+usable].reset_index(drop=True)\n",
    "    return df_out, np.vstack(Xs_all), np.vstack(Yh_all)\n",
    "\n",
    "def predict_open(df, scX, scY, narx):\n",
    "    X      = build_lagged(df)\n",
    "    Xs     = scX.transform(X)\n",
    "    y_pred = scY.inverse_transform(narx.predict(Xs, verbose=0))\n",
    "    return df.iloc[LAG+1:].reset_index(drop=True), Xs, y_pred\n",
    "\n",
    "# â”€â”€  QR nets & conformal deltas  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "QR = {}\n",
    "for col in STATE_COLS:\n",
    "    for q in (0.1, 0.9):\n",
    "        QR[(col, q)] = tf.keras.models.load_model(MODEL_ROOT/f'qr/{col}_{q:.1f}',\n",
    "                                                  compile=False)\n",
    "DELTAS = pickle.loads((MODEL_ROOT/'conformal_deltas.pkl').read_bytes())\n",
    "\n",
    "def add_cqr(df, Xs, base_pred, mode: str):\n",
    "    \"\"\"Attach point-pred + CQR bounds to DataFrame.\"\"\"\n",
    "    out = df.copy()\n",
    "    for i, col in enumerate(STATE_COLS):\n",
    "        lo = QR[(col, 0.1)].predict(Xs, verbose=0).flatten()\n",
    "        hi = QR[(col, 0.9)].predict(Xs, verbose=0).flatten()\n",
    "        out[f\"{col}_{mode}\"]    = base_pred[:, i]\n",
    "        out[f\"{col}_{mode}_lo\"] = base_pred[:, i] - lo - DELTAS[col]\n",
    "        out[f\"{col}_{mode}_hi\"] = base_pred[:, i] + hi + DELTAS[col]\n",
    "    return out\n",
    "\n",
    "# â”€â”€  Metrics helper  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def metric_table(df: pd.DataFrame, mode: str):\n",
    "    res = {}\n",
    "    for col in STATE_COLS:\n",
    "        y_true = df[col].values\n",
    "        y_pred = df[f\"{col}_{mode}\"].values\n",
    "        lo     = df[f\"{col}_{mode}_lo\"].values\n",
    "        hi     = df[f\"{col}_{mode}_hi\"].values\n",
    "        msk    = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "\n",
    "        res[f\"{col}_MAE\"] = mean_absolute_error(y_true[msk], y_pred[msk])\n",
    "        res[f\"{col}_MSE\"] = mean_squared_error(y_true[msk], y_pred[msk])\n",
    "        inside            = (y_true >= lo) & (y_true <= hi)\n",
    "        res[f\"{col}_COV\"] = 100. * inside[msk].mean()\n",
    "    return res\n",
    "\n",
    "# ----------  Plot helpers  -------------------------------------------------\n",
    "def plot_ts(df, out, mode):\n",
    "    t = np.arange(len(df))\n",
    "    for col in STATE_COLS:\n",
    "        plt.figure(figsize=(7,3))\n",
    "        plt.plot(t, df[col],  label='truth', lw=1)\n",
    "        plt.plot(t, df[f'{col}_{mode}'], label='pred', lw=1)\n",
    "        plt.fill_between(t, df[f'{col}_{mode}_lo'], df[f'{col}_{mode}_hi'],\n",
    "                         alpha=.25, label='90 % PI')\n",
    "        plt.title(col); plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(out/f\"{col}_{mode}.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "def plot_scatter(df, out, mode):\n",
    "    for col in STATE_COLS:\n",
    "        plt.figure(figsize=(3.5,3.5))\n",
    "        plt.scatter(df[col], df[f'{col}_{mode}'], s=8, alpha=.6)\n",
    "        mn, mx = df[[col, f'{col}_{mode}']].values.min(), \\\n",
    "                 df[[col, f'{col}_{mode}']].values.max()\n",
    "        plt.plot([mn, mx],[mn, mx],'r--'); plt.title(col); plt.tight_layout()\n",
    "        plt.savefig(out/f\"{col}_{mode}_scatter.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Main loop over test files\n",
    "# --------------------------------------------------------------------------\n",
    "summary = []\n",
    "for p in sorted(TEST_DIR.glob(\"*.txt\")):\n",
    "    stem  = p.stem\n",
    "    out_f = OUT_DIR / stem\n",
    "    out_f.mkdir(exist_ok=True)\n",
    "    print(f\"\\nâš™  Processing {stem} â€¦\")\n",
    "\n",
    "    try:\n",
    "        # 1. preprocess & cluster\n",
    "        df    = preprocess(p)\n",
    "        cid   = detect_cluster(df)\n",
    "        scX, scY, narx = load_cluster(cid)\n",
    "\n",
    "        # 2. open-loop\n",
    "        df_o, Xo_s, y_open = predict_open( df, scX, scY, narx)\n",
    "        df_o = add_cqr(df_o, Xo_s, y_open, mode=\"open\")\n",
    "\n",
    "        # 3. closed-loop\n",
    "        df_c, Xc_s, y_closed = predict_closed(df, scX, scY, narx, HORIZON)\n",
    "        df_c = add_cqr(df_c, Xc_s, y_closed, mode=\"closed\")\n",
    "\n",
    "        # 4. merge\n",
    "        df_pred = pd.concat(\n",
    "            [df_o,\n",
    "             df_c[[f\"{c}_{m}\" for c in STATE_COLS\n",
    "                               for m in (\"closed\", \"closed_lo\", \"closed_hi\")]]],\n",
    "            axis=1)\n",
    "\n",
    "        # 5. save & plots\n",
    "        df_pred.to_csv(out_f/\"predictions.csv\", index=False)\n",
    "        plot_ts(df_pred, out_f, mode=\"open\")\n",
    "        plot_ts(df_pred, out_f, mode=\"closed\")\n",
    "        plot_scatter(df_pred, out_f, mode=\"open\")\n",
    "        plot_scatter(df_pred, out_f, mode=\"closed\")\n",
    "\n",
    "        # 6. metrics\n",
    "        m_open   = metric_table(df_pred, mode=\"open\")\n",
    "        m_closed = metric_table(df_pred, mode=\"closed\")\n",
    "        summary.append(\n",
    "            {\"file\": stem, **m_open,\n",
    "                        **{f\"{k}_closed\": v for k,v in m_closed.items()}}\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"â¨¯  {stem} skipped  â†’  {e}\")\n",
    "\n",
    "# â”€â”€  Aggregate summary  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_sum = pd.DataFrame(summary)\n",
    "df_sum.to_csv(OUT_DIR/\"metrics_summary.csv\", index=False)\n",
    "\n",
    "rows = []\n",
    "for col in STATE_COLS:\n",
    "    rows.append([\n",
    "        col,\n",
    "        df_sum[f\"{col}_MAE\"].mean(),\n",
    "        df_sum[f\"{col}_MAE_closed\"].mean(),\n",
    "        df_sum[f\"{col}_MSE\"].mean(),\n",
    "        df_sum[f\"{col}_MSE_closed\"].mean(),\n",
    "        df_sum[f\"{col}_COV\"].mean(),\n",
    "        df_sum[f\"{col}_COV_closed\"].mean()\n",
    "    ])\n",
    "\n",
    "print(\"\\nğŸ“Š  Average error & coverage (open vs closed)\\n\")\n",
    "print(pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"Var\",\n",
    "                 \"Open MAE\", \"Closed MAE\",\n",
    "                 \"Open MSE\", \"Closed MSE\",\n",
    "                 \"Open COV %\", \"Closed COV %\"]\n",
    ").to_string(index=False, float_format=\"%.5f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd24a2-390d-46c8-bff6-5dff789321d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  LAG = 10,  horizon = 5\n",
      "\n",
      "âš™  Processing file_16361 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_20726 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_22636 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_3388 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_34890 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_39455 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_41551 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_49595 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_5325 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_54889 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_550 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_56035 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_62851 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_63816 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_68111 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_77484 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_82278 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_87603 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_96991 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš™  Processing file_9985 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average error & coverage (open vs closed)\n",
      "\n",
      " Var   Open MAE  Closed MAE   Open MSE  Closed MSE  Open COV %  Closed COV %\n",
      "T_PM 0.14411705  0.30596885 0.05252401  0.34400150 99.81294237   98.63959391\n",
      "   c 0.00030193  0.00062255 0.00000025  0.00000127 60.55611729   60.09137056\n",
      " d10 0.00010021  0.00010313 0.00000186  0.00000186  7.52780586    7.44162437\n",
      " d50 0.00010400  0.00010712 0.00000189  0.00000190 10.15166835   10.13197970\n",
      " d90 0.00011353  0.00011639 0.00000145  0.00000146 12.14863498   12.14720812\n",
      "T_TM 0.15961778  0.32704550 0.06174111  0.37979913 99.81294237   98.73604061\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "\n",
    "Generates open-loop and closed-loop forecasts for the SFC data, with calibrated\n",
    "CQR prediction bands + full metrics & plots.\n",
    "\n",
    "Mapping to Fig S5 (page 65) as per info given:\n",
    "    â€¢ \"prediction model\" in the paper  â†’  **closed-loop** here\n",
    "    â€¢ \"simulation model\"               â†’  **open-loop** here\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€  Imports  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# â”€â”€  Paths & runtime constants  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TEST_DIR   = Path(r\"C:/Users/nishi/Desktop/MLME Project/Data/Test\")\n",
    "MODEL_ROOT = Path(r\"C:/Users/nishi/Desktop/MLME Project/model_5files\")\n",
    "OUT_DIR    = MODEL_ROOT/'Results_CQR_5files'; OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ----------  Load metadata to stay in sync with training  -----------------\n",
    "\n",
    "# --- Unit configuration ---------------------------------------------------\n",
    "#USE_MICRONS = False        # True  âœ internally work in Âµm  \n",
    "PSD_COLS    = ('d10', 'd50', 'd90')\n",
    "\n",
    "\n",
    "meta         = json.loads((MODEL_ROOT/'metadata.json').read_text())\n",
    "STATE_COLS   = meta['state_cols']\n",
    "EXOG_COLS    = meta['exog_cols']\n",
    "LAG          = meta['lag']\n",
    "CLUST_COLS   = STATE_COLS + EXOG_COLS\n",
    "HORIZON      = 5         # closed-loop rollout length  (= Fig S5 horizon)\n",
    "\n",
    "print(f\"[INFO]  LAG = {LAG},  horizon = {HORIZON}\")\n",
    "\n",
    "# â”€â”€  Pre-processing helpers (same as training)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def read_txt(p): return pd.read_csv(p, sep='\\t', engine='python'\n",
    "                                   ).apply(pd.to_numeric, errors='coerce')\n",
    "def clean_df(df):\n",
    "    df = df.dropna(subset=CLUST_COLS)\n",
    "    df = df[(df.T_PM.between(250,400)) & (df.T_TM.between(250,400))\n",
    "            & (df.d10>0)&(df.d50>0)&(df.d90>0)\n",
    "            & (df.mf_PM>=0)&(df.mf_TM>=0)&(df.Q_g>=0)]\n",
    "    return df.reset_index(drop=True)\n",
    "#def harmonise_units(df):\n",
    "    \"\"\"\n",
    "    Make sure d10 / d50 / d90 are in *micrometres*.\n",
    "\n",
    "    Rule:\n",
    "        â€¢ If the median of a column is smaller than 1 Ã— 10â»Â² (i.e. < 1 cm)\n",
    "          the data must already be in metres  âœ  multiply by 1 Ã— 10â¶.\n",
    "        â€¢ Otherwise assume it is already Âµm and leave unchanged.\n",
    "\n",
    "    Works row-wise, so mixed units inside the same file are also fixed.\n",
    "    \"\"\"\n",
    "    if not USE_MICRONS:\n",
    "        return df    # fall-back for future experiments\n",
    "\n",
    "    for col in PSD_COLS:\n",
    "        median = df[col].median(skipna=True)\n",
    "        if median < 1e-2:         # < 1 cm  â‡’ data were metres\n",
    "            df[col] *= 1e6        # m â†’ Âµm\n",
    "    return df\n",
    "def to_metres(df):\n",
    "    \"\"\"\n",
    "    Ensure d10 / d50 / d90 are in metres, regardless of the fileâ€™s unit.\n",
    "\n",
    "    Heuristic:\n",
    "        â€¢ If the column median is > 0.01  (i.e. larger than 1 cm)\n",
    "          the numbers must be Âµm  â†’ divide by 1 Ã— 10â¶.\n",
    "        â€¢ Otherwise they are already metres â†’ leave untouched.\n",
    "    \"\"\"\n",
    "    for col in PSD_COLS:\n",
    "        if df[col].median(skipna=True) > 1e-2:   # > 1 cm â‡’ Âµm\n",
    "            df[col] /= 1e6                       # Âµm â†’ m\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(path: Path) -> pd.DataFrame:\n",
    "    df = clean_df(read_txt(path))\n",
    "    df = to_metres(df)        # <<< make sure we are in metres\n",
    "    return df\n",
    "\n",
    "\n",
    "# â”€â”€  Cluster artefacts  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sc_feat = pickle.loads((MODEL_ROOT/'feature_scaler.pkl').read_bytes())\n",
    "kmeans  = pickle.loads((MODEL_ROOT/'kmeans_model.pkl' ).read_bytes())\n",
    "\n",
    "def file_signature(df):\n",
    "   # Return same feature vector used during training clustering.\n",
    "    arr = df[CLUST_COLS].values\n",
    "    return np.concatenate([arr.mean(0), arr.std(0), arr.min(0), arr.max(0)]).reshape(1,-1)\n",
    "\n",
    "def detect_cluster(df) -> int:\n",
    "    return int(kmeans.predict(sc_feat.transform(file_signature(df)))[0])\n",
    "\n",
    "# â”€â”€  Build lag matrix (newest-to-oldest)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_lagged(df, lag=LAG):\n",
    "    rows = []\n",
    "    for i in range(lag, len(df)-1):          # need y_{t+1} for target\n",
    "        row = []\n",
    "        for l in range(0, lag+1):            # 0 â€¦ LAG\n",
    "            idx = i - l\n",
    "            row.extend(df[CLUST_COLS].iloc[idx].values)\n",
    "        rows.append(row)\n",
    "    return np.asarray(rows, np.float32)\n",
    "\n",
    "# â”€â”€  Load per-cluster artefacts  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_cluster(cid):\n",
    "    scX = pickle.loads((MODEL_ROOT/f'narx/scaler_X_{cid}.pkl').read_bytes())\n",
    "    scY = pickle.loads((MODEL_ROOT/f'narx/scaler_Y_{cid}.pkl').read_bytes())\n",
    "    narx = tf.keras.models.load_model(MODEL_ROOT/f'narx/cluster_{cid}',\n",
    "                                      compile=False)\n",
    "    return scX, scY, narx\n",
    "\n",
    "# â”€â”€  Closed-loop rollout  (scaled space)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def rollout(model, lag_scaled, horizon, scX, scY, exog_future_raw):\n",
    "    \"\"\"\n",
    "    Predict horizon-step ahead sequence *closed-loop*:\n",
    "        at each step feed back the *predicted* (scaled) state,\n",
    "        plus the *true* exogenous input for that step.\n",
    "    \"\"\"\n",
    "    x      = lag_scaled.copy()              # shape = (input_dim,)\n",
    "    preds  = []\n",
    "    n_y    = len(STATE_COLS)\n",
    "    n_u    = len(EXOG_COLS)\n",
    "    stride = n_y + n_u                      # one (y+u) block per time step\n",
    "\n",
    "    # handy slices\n",
    "    y_slice  = slice(0, n_y)                # first part of each block\n",
    "    u_slice  = slice(n_y, n_y+n_u)          # second part of each block\n",
    "\n",
    "    for k in range(horizon):\n",
    "        y_scaled  = model.predict(x[None], verbose=0)[0]     # (n_y,)\n",
    "        y_raw     = scY.inverse_transform(y_scaled[None])[0]\n",
    "        preds.append(y_raw)\n",
    "\n",
    "        # --- push history backwards (newest first layout) ---\n",
    "        x[stride:] = x[:-stride] * 1.0      # shift older blocks\n",
    "        x[y_slice] = y_scaled               # newest state (pred)\n",
    "        # scale & insert exogenous truth for step k\n",
    "        mu_u   = scX.mean_[u_slice]\n",
    "        sig_u  = scX.scale_[u_slice]\n",
    "        x[u_slice] = (exog_future_raw[k] - mu_u) / sig_u\n",
    "\n",
    "    return np.asarray(preds)\n",
    "\n",
    "def predict_closed(df, scX, scY, narx, horizon=HORIZON):\n",
    "    \"\"\"\n",
    "    Rolling closed-loop prediction with stride = 1.\n",
    "    Returns\n",
    "        df_out   â€“ ground-truth rows matching predictions\n",
    "        Xs_all   â€“ scaled lag vectors   (for QR nets)\n",
    "        Yh_all   â€“ raw predictions      (shape rows Ã— |STATE|)\n",
    "    \"\"\"\n",
    "    total   = len(df) - 1 - LAG\n",
    "    usable  = total - horizon + 1\n",
    "    Xs_all, Yh_all = [], []\n",
    "\n",
    "    for t0 in trange(usable, desc=\"closed\", leave=False):\n",
    "        # lag vector (newest-first)\n",
    "        row  = []\n",
    "        for l in range(0, LAG+1):\n",
    "            idx = t0 + LAG - l\n",
    "            row.extend(df[CLUST_COLS].iloc[idx].values)\n",
    "        lag_raw  = np.asarray(row, np.float32)\n",
    "        lag_s    = scX.transform(lag_raw[None])[0]\n",
    "        exog_f   = df[EXOG_COLS].iloc[t0+1 : t0+1+horizon].values\n",
    "        y_seq    = rollout(narx, lag_s, horizon, scX, scY, exog_f)\n",
    "        Xs_all.append(lag_s)\n",
    "        Yh_all.append(y_seq[-1])            # only last step (t+h)\n",
    "\n",
    "    df_out = df.iloc[LAG+horizon : LAG+horizon+usable].reset_index(drop=True)\n",
    "    return df_out, np.vstack(Xs_all), np.vstack(Yh_all)\n",
    "\n",
    "def predict_open(df, scX, scY, narx):\n",
    "    X      = build_lagged(df)\n",
    "    Xs     = scX.transform(X)\n",
    "    y_pred = scY.inverse_transform(narx.predict(Xs, verbose=0))\n",
    "    return df.iloc[LAG+1:].reset_index(drop=True), Xs, y_pred\n",
    "\n",
    "# â”€â”€  QR nets & conformal deltas  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "QR = {}\n",
    "for col in STATE_COLS:\n",
    "    for q in (0.1, 0.9):\n",
    "        QR[(col, q)] = tf.keras.models.load_model(MODEL_ROOT/f'qr/{col}_{q:.1f}',\n",
    "                                                  compile=False)\n",
    "DELTAS = pickle.loads((MODEL_ROOT/'conformal_deltas.pkl').read_bytes())\n",
    "\n",
    "def add_cqr(df, Xs, base_pred, mode: str):\n",
    "   # Attach point-pred + CQR bounds to DataFrame.\"\"\"\n",
    "    out = df.copy()\n",
    "    for i, col in enumerate(STATE_COLS):\n",
    "        lo = QR[(col, 0.1)].predict(Xs, verbose=0).flatten()\n",
    "        hi = QR[(col, 0.9)].predict(Xs, verbose=0).flatten()\n",
    "        out[f\"{col}_{mode}\"]    = base_pred[:, i]\n",
    "        out[f\"{col}_{mode}_lo\"] = base_pred[:, i] - lo - DELTAS[col]\n",
    "        out[f\"{col}_{mode}_hi\"] = base_pred[:, i] + hi + DELTAS[col]\n",
    "    return out\n",
    "\n",
    "# â”€â”€  Metrics helper  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def metric_table(df: pd.DataFrame, mode: str):\n",
    "    res = {}\n",
    "    for col in STATE_COLS:\n",
    "        y_true = df[col].values\n",
    "        y_pred = df[f\"{col}_{mode}\"].values\n",
    "        lo     = df[f\"{col}_{mode}_lo\"].values\n",
    "        hi     = df[f\"{col}_{mode}_hi\"].values\n",
    "        msk    = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "\n",
    "        res[f\"{col}_MAE\"] = mean_absolute_error(y_true[msk], y_pred[msk])\n",
    "        res[f\"{col}_MSE\"] = mean_squared_error(y_true[msk], y_pred[msk])\n",
    "        inside            = (y_true >= lo) & (y_true <= hi)\n",
    "        res[f\"{col}_COV\"] = 100. * inside[msk].mean()\n",
    "    return res\n",
    "\n",
    "# ----------  Plot helpers  -------------------------------------------------\n",
    "def plot_ts(df, out, mode):\n",
    "    t = np.arange(len(df))\n",
    "    for col in STATE_COLS:\n",
    "        plt.figure(figsize=(7,3))\n",
    "        plt.plot(t, df[col],  label='truth', lw=1)\n",
    "        plt.plot(t, df[f'{col}_{mode}'], label='pred', lw=1)\n",
    "        plt.fill_between(t, df[f'{col}_{mode}_lo'], df[f'{col}_{mode}_hi'],\n",
    "                         alpha=.25, label='90 % PI')\n",
    "        plt.title(col); plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(out/f\"{col}_{mode}.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "def plot_scatter(df, out, mode):\n",
    "    for col in STATE_COLS:\n",
    "        plt.figure(figsize=(3.5,3.5))\n",
    "        plt.scatter(df[col], df[f'{col}_{mode}'], s=8, alpha=.6)\n",
    "        mn, mx = df[[col, f'{col}_{mode}']].values.min(), \\\n",
    "                 df[[col, f'{col}_{mode}']].values.max()\n",
    "        plt.plot([mn, mx],[mn, mx],'r--'); plt.title(col); plt.tight_layout()\n",
    "        plt.savefig(out/f\"{col}_{mode}_scatter.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Main loop over test files\n",
    "# --------------------------------------------------------------------------\n",
    "summary = []\n",
    "for p in sorted(TEST_DIR.glob(\"*.txt\")):\n",
    "    stem  = p.stem\n",
    "    out_f = OUT_DIR / stem\n",
    "    out_f.mkdir(exist_ok=True)\n",
    "    print(f\"\\nâš™  Processing {stem} â€¦\")\n",
    "\n",
    "    try:\n",
    "        # 1. preprocess & cluster\n",
    "        df    = preprocess(p)\n",
    "        cid   = detect_cluster(df)\n",
    "        scX, scY, narx = load_cluster(cid)\n",
    "\n",
    "        # 2. open-loop\n",
    "        df_o, Xo_s, y_open = predict_open( df, scX, scY, narx)\n",
    "        df_o = add_cqr(df_o, Xo_s, y_open, mode=\"open\")\n",
    "\n",
    "        # 3. closed-loop\n",
    "        df_c, Xc_s, y_closed = predict_closed(df, scX, scY, narx, HORIZON)\n",
    "        df_c = add_cqr(df_c, Xc_s, y_closed, mode=\"closed\")\n",
    "\n",
    "        # 4. merge\n",
    "        df_pred = pd.concat(\n",
    "            [df_o,\n",
    "             df_c[[f\"{c}_{m}\" for c in STATE_COLS\n",
    "                               for m in (\"closed\", \"closed_lo\", \"closed_hi\")]]],\n",
    "            axis=1)\n",
    "\n",
    "        # 5. save & plots\n",
    "        df_pred.to_csv(out_f/\"predictions.csv\", index=False)\n",
    "        plot_ts(df_pred, out_f, mode=\"open\")\n",
    "        plot_ts(df_pred, out_f, mode=\"closed\")\n",
    "        plot_scatter(df_pred, out_f, mode=\"open\")\n",
    "        plot_scatter(df_pred, out_f, mode=\"closed\")\n",
    "\n",
    "        # 6. metrics\n",
    "        m_open   = metric_table(df_pred, mode=\"open\")\n",
    "        m_closed = metric_table(df_pred, mode=\"closed\")\n",
    "        summary.append(\n",
    "            {\"file\": stem, **m_open,\n",
    "                        **{f\"{k}_closed\": v for k,v in m_closed.items()}}\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"â¨¯  {stem} skipped  â†’  {e}\")\n",
    "\n",
    "# â”€â”€  Aggregate summary  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_sum = pd.DataFrame(summary)\n",
    "df_sum.to_csv(OUT_DIR/\"metrics_summary.csv\", index=False)\n",
    "\n",
    "rows = []\n",
    "for col in STATE_COLS:\n",
    "    rows.append([\n",
    "        col,\n",
    "        df_sum[f\"{col}_MAE\"].mean(),\n",
    "        df_sum[f\"{col}_MAE_closed\"].mean(),\n",
    "        df_sum[f\"{col}_MSE\"].mean(),\n",
    "        df_sum[f\"{col}_MSE_closed\"].mean(),\n",
    "        df_sum[f\"{col}_COV\"].mean(),\n",
    "        df_sum[f\"{col}_COV_closed\"].mean()\n",
    "    ])\n",
    "\n",
    "print(\"\\n Average error & coverage (open vs closed)\\n\")\n",
    "print(pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"Var\",\n",
    "                 \"Open MAE\", \"Closed MAE\",\n",
    "                 \"Open MSE\", \"Closed MSE\",\n",
    "                 \"Open COV %\", \"Closed COV %\"]\n",
    ").to_string(index=False, float_format=\"%0.8f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbbe9a-c2fb-4ab3-a46f-2b4be6f8c09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  LAG = 10,  horizon = 5\n",
      "\n",
      "âš™  Processing file_12738 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average error & coverage (open vs closed)\n",
      "\n",
      " Var   Open MAE  Closed MAE   Open MSE  Closed MSE   Open COV %  Closed COV %\n",
      "T_PM 0.11271435  0.22819815 0.03088757  0.22546750 100.00000000   98.88324873\n",
      "   c 0.00019928  0.00045378 0.00000012  0.00000056  81.90091001   81.42131980\n",
      " d10 0.00010081  0.00010187 0.00000078  0.00000078  19.31243680   19.39086294\n",
      " d50 0.00004028  0.00004010 0.00000001  0.00000001  14.15571284   14.21319797\n",
      " d90 0.00011414  0.00011434 0.00000154  0.00000155  10.81900910   10.86294416\n",
      "T_TM 0.10957127  0.22187008 0.02948032  0.24022710 100.00000000   98.78172589\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "\n",
    "Generates open-loop and closed-loop forecasts for the SFC data, with calibrated\n",
    "CQR prediction bands + full metrics & plots.\n",
    "\n",
    "Mapping to Fig S5 (page 65) as per info given:\n",
    "    â€¢ \"prediction model\" in the paper  â†’  **closed-loop** here\n",
    "    â€¢ \"simulation model\"               â†’  **open-loop** here\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€  Imports  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# â”€â”€  Paths & runtime constants  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TEST_DIR   = Path(r\"C:/Users/nishi/Desktop/MLME Project/Beat-the-Felix\")\n",
    "MODEL_ROOT = Path(r\"C:/Users/nishi/Desktop/MLME Project/model_5files\")\n",
    "OUT_DIR    = MODEL_ROOT/'BEAT'; OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ----------  Load metadata to stay in sync with training  -----------------\n",
    "\n",
    "# --- Unit configuration ---------------------------------------------------\n",
    "#USE_MICRONS = False        # True  âœ internally work in Âµm  \n",
    "PSD_COLS    = ('d10', 'd50', 'd90')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meta         = json.loads((MODEL_ROOT/'metadata.json').read_text())\n",
    "STATE_COLS   = meta['state_cols']\n",
    "EXOG_COLS    = meta['exog_cols']\n",
    "LAG          = meta['lag']\n",
    "CLUST_COLS   = STATE_COLS + EXOG_COLS\n",
    "HORIZON      = 5         # closed-loop rollout length  (= Fig S5 horizon)\n",
    "\n",
    "print(f\"[INFO]  LAG = {LAG},  horizon = {HORIZON}\")\n",
    "\n",
    "# â”€â”€  Pre-processing helpers (same as training)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def read_txt(p): return pd.read_csv(p, sep='\\t', engine='python'\n",
    "                                   ).apply(pd.to_numeric, errors='coerce')\n",
    "def clean_df(df):\n",
    "    df = df.dropna(subset=CLUST_COLS)\n",
    "    df = df[(df.T_PM.between(250,400)) & (df.T_TM.between(250,400))\n",
    "            & (df.d10>0)&(df.d50>0)&(df.d90>0)\n",
    "            & (df.mf_PM>=0)&(df.mf_TM>=0)&(df.Q_g>=0)]\n",
    "    return df.reset_index(drop=True)\n",
    "#def harmonise_units(df):\n",
    "    \"\"\"\n",
    "    Make sure d10 / d50 / d90 are in *micrometres*.\n",
    "\n",
    "    Rule:\n",
    "        â€¢ If the median of a column is smaller than 1 Ã— 10â»Â² (i.e. < 1 cm)\n",
    "          the data must already be in metres  âœ  multiply by 1 Ã— 10â¶.\n",
    "        â€¢ Otherwise assume it is already Âµm and leave unchanged.\n",
    "\n",
    "    Works row-wise, so mixed units inside the same file are also fixed.\n",
    "    \"\"\"\n",
    "    if not USE_MICRONS:\n",
    "        return df    # fall-back for future experiments\n",
    "\n",
    "    for col in PSD_COLS:\n",
    "        median = df[col].median(skipna=True)\n",
    "        if median < 1e-2:         # < 1 cm  â‡’ data were metres\n",
    "            df[col] *= 1e6        # m â†’ Âµm\n",
    "    return df\n",
    "def to_metres(df):\n",
    "    \"\"\"\n",
    "    Ensure d10 / d50 / d90 are in metres, regardless of the fileâ€™s unit.\n",
    "\n",
    "    Heuristic:\n",
    "        â€¢ If the column median is > 0.01  (i.e. larger than 1 cm)\n",
    "          the numbers must be Âµm  â†’ divide by 1 Ã— 10â¶.\n",
    "        â€¢ Otherwise they are already metres â†’ leave untouched.\n",
    "    \"\"\"\n",
    "    for col in PSD_COLS:\n",
    "        if df[col].median(skipna=True) > 1e-2:   # > 1 cm â‡’ Âµm\n",
    "            df[col] /= 1e6                       # Âµm â†’ m\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(path: Path) -> pd.DataFrame:\n",
    "    df = clean_df(read_txt(path))\n",
    "    df = to_metres(df)        # <<< make sure we are in metres\n",
    "    return df\n",
    "\n",
    "\n",
    "# â”€â”€  Cluster artefacts  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sc_feat = pickle.loads((MODEL_ROOT/'feature_scaler.pkl').read_bytes())\n",
    "kmeans  = pickle.loads((MODEL_ROOT/'kmeans_model.pkl' ).read_bytes())\n",
    "\n",
    "def file_signature(df):\n",
    "   # Return same feature vector used during training clustering.\n",
    "    arr = df[CLUST_COLS].values\n",
    "    return np.concatenate([arr.mean(0), arr.std(0), arr.min(0), arr.max(0)]).reshape(1,-1)\n",
    "\n",
    "def detect_cluster(df) -> int:\n",
    "    return int(kmeans.predict(sc_feat.transform(file_signature(df)))[0])\n",
    "\n",
    "# â”€â”€  Build lag matrix (newest-to-oldest)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_lagged(df, lag=LAG):\n",
    "    rows = []\n",
    "    for i in range(lag, len(df)-1):          # need y_{t+1} for target\n",
    "        row = []\n",
    "        for l in range(0, lag+1):            # 0 â€¦ LAG\n",
    "            idx = i - l\n",
    "            row.extend(df[CLUST_COLS].iloc[idx].values)\n",
    "        rows.append(row)\n",
    "    return np.asarray(rows, np.float32)\n",
    "\n",
    "# â”€â”€  Load per-cluster artefacts  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_cluster(cid):\n",
    "    scX = pickle.loads((MODEL_ROOT/f'narx/scaler_X_{cid}.pkl').read_bytes())\n",
    "    scY = pickle.loads((MODEL_ROOT/f'narx/scaler_Y_{cid}.pkl').read_bytes())\n",
    "    narx = tf.keras.models.load_model(MODEL_ROOT/f'narx/cluster_{cid}',\n",
    "                                      compile=False)\n",
    "    return scX, scY, narx\n",
    "\n",
    "# â”€â”€  Closed-loop rollout  (scaled space)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def rollout(model, lag_scaled, horizon, scX, scY, exog_future_raw):\n",
    "    \"\"\"\n",
    "    Predict horizon-step ahead sequence *closed-loop*:\n",
    "        at each step feed back the *predicted* (scaled) state,\n",
    "        plus the *true* exogenous input for that step.\n",
    "    \"\"\"\n",
    "    x      = lag_scaled.copy()              # shape = (input_dim,)\n",
    "    preds  = []\n",
    "    n_y    = len(STATE_COLS)\n",
    "    n_u    = len(EXOG_COLS)\n",
    "    stride = n_y + n_u                      # one (y+u) block per time step\n",
    "\n",
    "    # handy slices\n",
    "    y_slice  = slice(0, n_y)                # first part of each block\n",
    "    u_slice  = slice(n_y, n_y+n_u)          # second part of each block\n",
    "\n",
    "    for k in range(horizon):\n",
    "        y_scaled  = model.predict(x[None], verbose=0)[0]     # (n_y,)\n",
    "        y_raw     = scY.inverse_transform(y_scaled[None])[0]\n",
    "        preds.append(y_raw)\n",
    "\n",
    "        # --- push history backwards (newest first layout) ---\n",
    "        x[stride:] = x[:-stride] * 1.0      # shift older blocks\n",
    "        x[y_slice] = y_scaled               # newest state (pred)\n",
    "        # scale & insert exogenous truth for step k\n",
    "        mu_u   = scX.mean_[u_slice]\n",
    "        sig_u  = scX.scale_[u_slice]\n",
    "        x[u_slice] = (exog_future_raw[k] - mu_u) / sig_u\n",
    "\n",
    "    return np.asarray(preds)\n",
    "\n",
    "def predict_closed(df, scX, scY, narx, horizon=HORIZON):\n",
    "    \"\"\"\n",
    "    Rolling closed-loop prediction with stride = 1.\n",
    "    Returns\n",
    "        df_out   â€“ ground-truth rows matching predictions\n",
    "        Xs_all   â€“ scaled lag vectors   (for QR nets)\n",
    "        Yh_all   â€“ raw predictions      (shape rows Ã— |STATE|)\n",
    "    \"\"\"\n",
    "    total   = len(df) - 1 - LAG\n",
    "    usable  = total - horizon + 1\n",
    "    Xs_all, Yh_all = [], []\n",
    "\n",
    "    for t0 in trange(usable, desc=\"closed\", leave=False):\n",
    "        # lag vector (newest-first)\n",
    "        row  = []\n",
    "        for l in range(0, LAG+1):\n",
    "            idx = t0 + LAG - l\n",
    "            row.extend(df[CLUST_COLS].iloc[idx].values)\n",
    "        lag_raw  = np.asarray(row, np.float32)\n",
    "        lag_s    = scX.transform(lag_raw[None])[0]\n",
    "        exog_f   = df[EXOG_COLS].iloc[t0+1 : t0+1+horizon].values\n",
    "        y_seq    = rollout(narx, lag_s, horizon, scX, scY, exog_f)\n",
    "        Xs_all.append(lag_s)\n",
    "        Yh_all.append(y_seq[-1])            # only last step (t+h)\n",
    "\n",
    "    df_out = df.iloc[LAG+horizon : LAG+horizon+usable].reset_index(drop=True)\n",
    "    return df_out, np.vstack(Xs_all), np.vstack(Yh_all)\n",
    "\n",
    "def predict_open(df, scX, scY, narx):\n",
    "    X      = build_lagged(df)\n",
    "    Xs     = scX.transform(X)\n",
    "    y_pred = scY.inverse_transform(narx.predict(Xs, verbose=0))\n",
    "    return df.iloc[LAG+1:].reset_index(drop=True), Xs, y_pred\n",
    "\n",
    "# â”€â”€  QR nets & conformal deltas  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "QR = {}\n",
    "for col in STATE_COLS:\n",
    "    for q in (0.1, 0.9):\n",
    "        QR[(col, q)] = tf.keras.models.load_model(MODEL_ROOT/f'qr/{col}_{q:.1f}',\n",
    "                                                  compile=False)\n",
    "DELTAS = pickle.loads((MODEL_ROOT/'conformal_deltas.pkl').read_bytes())\n",
    "# Optional: Adjust deltas to improve coverage\n",
    "DELTAS['c']    *= 1.3\n",
    "DELTAS['d10']  *= 2.5\n",
    "DELTAS['d50']  *= 2.5\n",
    "DELTAS['d90']  *= 2.5\n",
    "\n",
    "\n",
    "def add_cqr(df, Xs, base_pred, mode: str):\n",
    "   # Attach point-pred + CQR bounds to DataFrame.\"\"\"\n",
    "    out = df.copy()\n",
    "    for i, col in enumerate(STATE_COLS):\n",
    "        lo = QR[(col, 0.1)].predict(Xs, verbose=0).flatten()\n",
    "        hi = QR[(col, 0.9)].predict(Xs, verbose=0).flatten()\n",
    "        out[f\"{col}_{mode}\"]    = base_pred[:, i]\n",
    "        out[f\"{col}_{mode}_lo\"] = base_pred[:, i] - lo - DELTAS[col]\n",
    "        out[f\"{col}_{mode}_hi\"] = base_pred[:, i] + hi + DELTAS[col]\n",
    "    return out\n",
    "\n",
    "# â”€â”€  Metrics helper  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def metric_table(df: pd.DataFrame, mode: str):\n",
    "    res = {}\n",
    "    for col in STATE_COLS:\n",
    "        y_true = df[col].values\n",
    "        y_pred = df[f\"{col}_{mode}\"].values\n",
    "        lo     = df[f\"{col}_{mode}_lo\"].values\n",
    "        hi     = df[f\"{col}_{mode}_hi\"].values\n",
    "        msk    = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "\n",
    "        res[f\"{col}_MAE\"] = mean_absolute_error(y_true[msk], y_pred[msk])\n",
    "        res[f\"{col}_MSE\"] = mean_squared_error(y_true[msk], y_pred[msk])\n",
    "        inside            = (y_true >= lo) & (y_true <= hi)\n",
    "        res[f\"{col}_COV\"] = 100. * inside[msk].mean()\n",
    "    return res\n",
    "\n",
    "# ----------  Plot helpers  -------------------------------------------------\n",
    "def plot_ts(df, out, mode):\n",
    "    t = np.arange(len(df))\n",
    "    for col in STATE_COLS:\n",
    "        plt.figure(figsize=(7,3))\n",
    "        plt.plot(t, df[col],  label='truth', lw=1)\n",
    "        plt.plot(t, df[f'{col}_{mode}'], label='pred', lw=1)\n",
    "        plt.fill_between(t, df[f'{col}_{mode}_lo'], df[f'{col}_{mode}_hi'],\n",
    "                         alpha=.25, label='90 % PI')\n",
    "        plt.title(col); plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(out/f\"{col}_{mode}.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "def plot_scatter(df, out, mode):\n",
    "    for col in STATE_COLS:\n",
    "        plt.figure(figsize=(3.5,3.5))\n",
    "        plt.scatter(df[col], df[f'{col}_{mode}'], s=8, alpha=.6)\n",
    "        mn, mx = df[[col, f'{col}_{mode}']].values.min(), \\\n",
    "                 df[[col, f'{col}_{mode}']].values.max()\n",
    "        plt.plot([mn, mx],[mn, mx],'r--'); plt.title(col); plt.tight_layout()\n",
    "        plt.savefig(out/f\"{col}_{mode}_scatter.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Main loop over test files\n",
    "# --------------------------------------------------------------------------\n",
    "summary = []\n",
    "for p in sorted(TEST_DIR.glob(\"*.txt\")):\n",
    "    stem  = p.stem\n",
    "    out_f = OUT_DIR / stem\n",
    "    out_f.mkdir(exist_ok=True)\n",
    "    print(f\"\\nâš™  Processing {stem} â€¦\")\n",
    "\n",
    "    try:\n",
    "        # 1. preprocess & cluster\n",
    "        df    = preprocess(p)\n",
    "        cid   = detect_cluster(df)\n",
    "        scX, scY, narx = load_cluster(cid)\n",
    "\n",
    "        # 2. open-loop\n",
    "        df_o, Xo_s, y_open = predict_open( df, scX, scY, narx)\n",
    "        df_o = add_cqr(df_o, Xo_s, y_open, mode=\"open\")\n",
    "\n",
    "        # 3. closed-loop\n",
    "        df_c, Xc_s, y_closed = predict_closed(df, scX, scY, narx, HORIZON)\n",
    "        df_c = add_cqr(df_c, Xc_s, y_closed, mode=\"closed\")\n",
    "\n",
    "        # 4. merge\n",
    "        df_pred = pd.concat(\n",
    "            [df_o,\n",
    "             df_c[[f\"{c}_{m}\" for c in STATE_COLS\n",
    "                               for m in (\"closed\", \"closed_lo\", \"closed_hi\")]]],\n",
    "            axis=1)\n",
    "\n",
    "        # 5. save & plots\n",
    "        df_pred.to_csv(out_f/\"predictions.csv\", index=False)\n",
    "        plot_ts(df_pred, out_f, mode=\"open\")\n",
    "        plot_ts(df_pred, out_f, mode=\"closed\")\n",
    "        plot_scatter(df_pred, out_f, mode=\"open\")\n",
    "        plot_scatter(df_pred, out_f, mode=\"closed\")\n",
    "\n",
    "        # 6. metrics\n",
    "        m_open   = metric_table(df_pred, mode=\"open\")\n",
    "        m_closed = metric_table(df_pred, mode=\"closed\")\n",
    "        summary.append(\n",
    "            {\"file\": stem, **m_open,\n",
    "                        **{f\"{k}_closed\": v for k,v in m_closed.items()}}\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"â¨¯  {stem} skipped  â†’  {e}\")\n",
    "\n",
    "# â”€â”€  Aggregate summary  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_sum = pd.DataFrame(summary)\n",
    "df_sum.to_csv(OUT_DIR/\"metrics_summary.csv\", index=False)\n",
    "\n",
    "rows = []\n",
    "for col in STATE_COLS:\n",
    "    rows.append([\n",
    "        col,\n",
    "        df_sum[f\"{col}_MAE\"].mean(),\n",
    "        df_sum[f\"{col}_MAE_closed\"].mean(),\n",
    "        df_sum[f\"{col}_MSE\"].mean(),\n",
    "        df_sum[f\"{col}_MSE_closed\"].mean(),\n",
    "        df_sum[f\"{col}_COV\"].mean(),\n",
    "        df_sum[f\"{col}_COV_closed\"].mean()\n",
    "    ])\n",
    "\n",
    "print(\"\\n Average error & coverage (open vs closed)\\n\")\n",
    "print(pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"Var\",\n",
    "                 \"Open MAE\", \"Closed MAE\",\n",
    "                 \"Open MSE\", \"Closed MSE\",\n",
    "                 \"Open COV %\", \"Closed COV %\"]\n",
    ").to_string(index=False, float_format=\"%0.8f\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
