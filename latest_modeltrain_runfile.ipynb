{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9b20f-026b-4889-9483-bb36a99e3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nishi\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training per-cluster NARX models …\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\narx\\cluster_0\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\narx\\cluster_0\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\narx\\cluster_0\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\narx\\cluster_0\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\narx\\cluster_0\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\narx\\cluster_1\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\narx\\cluster_1\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\narx\\cluster_1\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\narx\\cluster_1\\assets\n",
      " NARX training done.\n",
      "\n",
      " Training QR nets …\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\T_PM_0.1\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\T_PM_0.9\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\c_0.1\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\c_0.9\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\d10_0.1\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\d10_0.9\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\d50_0.1\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\d50_0.9\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\d90_0.1\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\d90_0.9\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\T_TM_0.1\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\\qr\\T_TM_0.9\\assets\n",
      " QR nets done.\n",
      "\n",
      " Computing conformal deltas …\n",
      " Saved deltas: {'T_PM': 2.437530517578125, 'c': 0.006572812795639038, 'd10': 0.00013089957064948976, 'd50': 9.576865704730153e-05, 'd90': 0.00012195133604109287, 'T_TM': 2.497894287109375}\n",
      "\n",
      " Finished training. Model at: C:\\Users\\nishi\\Desktop\\MLME Project\\model_5files\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "\n",
    "End-to-end training script for the SFC project (Machine Learning Methods for\n",
    "Engineers, SS 25).\n",
    "\n",
    "Key features\n",
    "------------\n",
    "* Per-cluster NARX models\n",
    "* Input order:  [y_t , u_t , y_{t-1} , u_{t-1} , … , y_{t-LAG}]\n",
    "* Units: **metres** for d10/d50/d90 everywhere  ➜  no µm scaling headaches\n",
    "* Early-Stopping + Checkpoint to avoid over-/under-fitting\n",
    "* Weighted MSE  (PSD columns get higher loss weight)\n",
    "* Quantile Regression nets for τ = 0.1 & 0.9  +  Conformal deltas for 90 % PI\n",
    "  ─→  calibrated CQR intervals\n",
    "* Deterministic seeds  ➜  reproducible runs\n",
    "\n",
    " spec (see CrysID_MLME25.pdf).\n",
    "\"\"\"\n",
    "\n",
    "# ──  Imports  ──────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, random, json, pickle, shutil\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, optimizers, Sequential, callbacks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- Unit configuration ---------------------------------------------------\n",
    "#USE_MICRONS = False        # True  ➜ internally work in µm  (recommended by you)\n",
    "PSD_COLS    = ('d10', 'd50', 'd90')\n",
    "\n",
    "\n",
    "# ──  Reproducibility  ──────────────────────────────────────────────────────\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ──  User paths & global constants  ────────────────────────────────────────\n",
    "RAW_ROOT   = Path(r\"C:/Users/nishi/Desktop/MLME Project/Data/RAW DATA\")\n",
    "MODEL_DIR  = Path(r\"C:/Users/nishi/Desktop/MLME Project/model_5files\")  # <-- NEW\n",
    "# clean slate (avoids shape mismatches when you change LAG etc.)\n",
    "if MODEL_DIR.exists():\n",
    "    shutil.rmtree(MODEL_DIR)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(MODEL_DIR/\"narx\").mkdir()\n",
    "(MODEL_DIR/\"qr\").mkdir()\n",
    "\n",
    "LAG             = 10                # number of past steps\n",
    "N_CLUSTERS      = 2\n",
    "EPOCHS_NARX     = 70\n",
    "EPOCHS_QR       = 40\n",
    "BATCH_SIZE      = 32\n",
    "QUANTILES       = [0.1, 0.9]\n",
    "OUTPUT_WEIGHTS  = np.array([10, 6, 15, 15, 15, 10], dtype=\"float32\")  # higher weight to PSD\n",
    "\n",
    "# Column layout  (matches report)\n",
    "STATE_COLS = ['T_PM', 'c', 'd10', 'd50', 'd90', 'T_TM']\n",
    "EXOG_COLS  = ['mf_PM', 'mf_TM', 'Q_g', 'w_crystal',\n",
    "              'c_in', 'T_PM_in', 'T_TM_in']\n",
    "OUTPUT_COLS = STATE_COLS\n",
    "CLUST_COLS  = STATE_COLS + EXOG_COLS\n",
    "\n",
    "# ──  Helper functions  ─────────────────────────────────────────────────────\n",
    "def read_txt(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read TAB-separated text file into DataFrame (all numeric).\"\"\"\n",
    "    return pd.read_csv(path, sep='\\t', engine='python'\n",
    "                      ).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "def clean_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop NaNs + obvious sensor out-of-range artefacts.\"\"\"\n",
    "    df = df.dropna(subset=CLUST_COLS)\n",
    "    df = df[(df.T_PM.between(250, 400)) & (df.T_TM.between(250, 400))\n",
    "            & (df.d10>0) & (df.d50>0) & (df.d90>0)\n",
    "            & (df.mf_PM>=0) & (df.mf_TM>=0) & (df.Q_g>=0)]\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def to_metres(df):\n",
    "    \"\"\"\n",
    "    Ensure d10 / d50 / d90 are in metres, regardless of the file’s unit.\n",
    "\n",
    "    Heuristic:\n",
    "        • If the column median is > 0.01  (i.e. larger than 1 cm)\n",
    "          the numbers must be µm  → divide by 1 × 10⁶.\n",
    "        • Otherwise they are already metres → leave untouched.\n",
    "    \"\"\"\n",
    "    for col in PSD_COLS:\n",
    "        if df[col].median(skipna=True) > 1e-2:   # > 1 cm ⇒ µm\n",
    "            df[col] /= 1e6                       # µm → m\n",
    "    return df\n",
    "\n",
    "#def harmonise_units(df):\n",
    "    \"\"\"\n",
    "    Make sure d10 / d50 / d90 are in *micrometres*.\n",
    "\n",
    "    Rule:\n",
    "        • If the median of a column is smaller than 1 × 10⁻² (i.e. < 1 cm)\n",
    "          the data must already be in metres  ➜  multiply by 1 × 10⁶.\n",
    "        • Otherwise assume it is already µm and leave unchanged.\n",
    "\n",
    "    Works row-wise, so mixed units inside the same file are also fixed.\n",
    "    \"\"\"\n",
    "    if not USE_MICRONS:\n",
    "        return df    # fall-back for future experiments\n",
    "\n",
    "    for col in PSD_COLS:\n",
    "        median = df[col].median(skipna=True)\n",
    "        if median < 1e-2:         # < 1 cm  ⇒ data were metres\n",
    "            df[col] *= 1e6        # m → µm\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(path: Path) -> pd.DataFrame:\n",
    "    df = clean_df(read_txt(path))\n",
    "    df = to_metres(df)        # <<< make sure we are in metres\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------  Lag-matrix creation (newest-to-oldest)  ----------------------\n",
    "def make_xy(df: pd.DataFrame, lag=LAG\n",
    "           ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build NARX design-matrix X and target Y.\n",
    "\n",
    "    X row format (newest-first):\n",
    "        [ y_t, u_t, y_{t-1}, u_{t-1}, … , y_{t-lag}, u_{t-lag} ]\n",
    "    \"\"\"\n",
    "    hist_size = len(STATE_COLS) + len(EXOG_COLS)\n",
    "    X, Y = [], []\n",
    "    for i in range(lag, len(df)-1):\n",
    "        # newest-to-oldest slice\n",
    "        hist = []\n",
    "        for l in range(0, lag+1):                          # 0 … LAG\n",
    "            idx = i - l\n",
    "            hist.extend(df[STATE_COLS + EXOG_COLS].iloc[idx].values)\n",
    "        X.append(hist)\n",
    "        Y.append(df[STATE_COLS].iloc[i+1].values)\n",
    "    return np.asarray(X, np.float32), np.asarray(Y, np.float32)\n",
    "\n",
    "# ----------  Custom losses / builders  ------------------------------------\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    \"\"\"MSE with per-output weights (PSD columns matter more).\"\"\"\n",
    "    w = tf.constant(OUTPUT_WEIGHTS, dtype=y_true.dtype)\n",
    "    return tf.reduce_mean(tf.reduce_mean(tf.square(y_true - y_pred) * w, axis=-1))\n",
    "\n",
    "def build_narx(input_dim: int, output_dim: int) -> tf.keras.Model:\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(output_dim)\n",
    "    ])\n",
    "\n",
    "def build_qr(input_dim: int) -> tf.keras.Model:\n",
    "    return Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64 , activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def pinball_loss(tau: float):\n",
    "    \"\"\"Pinball / quantile loss for τ.\"\"\"\n",
    "    def loss(y, y_hat):\n",
    "        e = y - y_hat\n",
    "        return tf.reduce_mean(tf.maximum(tau * e, (tau - 1) * e))\n",
    "    return loss\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 1. Split RAW files once into *train* and *calibration* sub-folders.\n",
    "# --------------------------------------------------------------------------\n",
    "train_dir, calib_dir = RAW_ROOT/'train', RAW_ROOT/'calib'\n",
    "if not train_dir.exists():\n",
    "    train_dir.mkdir(); calib_dir.mkdir()\n",
    "    files = list(RAW_ROOT.glob(\"*.txt\"))\n",
    "    random.shuffle(files)\n",
    "    n_cal = int(0.2 * len(files))\n",
    "    for p in files[n_cal:]: shutil.copy(p, train_dir/p.name)\n",
    "    for p in files[:n_cal]: shutil.copy(p, calib_dir/p.name)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 2. Unsupervised clustering on summary stats  ➜  k-means labels.\n",
    "# --------------------------------------------------------------------------\n",
    "train_files = sorted(train_dir.glob(\"*.txt\"))\n",
    "\n",
    "feat = []\n",
    "for p in train_files:\n",
    "    df  = preprocess(p)\n",
    "    arr = df[CLUST_COLS].values\n",
    "    feat.append(np.concatenate([arr.mean(0), arr.std(0), arr.min(0), arr.max(0)]))\n",
    "feat = np.vstack(feat)\n",
    "\n",
    "sc_feat = StandardScaler().fit(feat)\n",
    "feat_s  = sc_feat.transform(feat)\n",
    "kmeans  = KMeans(n_clusters=N_CLUSTERS, random_state=SEED).fit(feat_s)\n",
    "\n",
    "pickle.dump(sc_feat, (MODEL_DIR/'feature_scaler.pkl').open('wb'))\n",
    "pickle.dump(kmeans , (MODEL_DIR/'kmeans_model.pkl' ).open('wb'))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 3. Train a separate NARX per cluster (with scaler per cluster).\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n Training per-cluster NARX models …\")\n",
    "for cid in range(N_CLUSTERS):\n",
    "    Xc, Yc = [], []\n",
    "    for idx, p in enumerate(train_files):\n",
    "        if kmeans.labels_[idx] != cid:\n",
    "            continue\n",
    "        x, y = make_xy(preprocess(p))\n",
    "        if len(x):\n",
    "            Xc.append(x); Yc.append(y)\n",
    "    if not Xc:\n",
    "        continue\n",
    "\n",
    "    Xc = np.vstack(Xc);  Yc = np.vstack(Yc)\n",
    "    scX = StandardScaler().fit(Xc)\n",
    "    scY = StandardScaler().fit(Yc)\n",
    "\n",
    "    pickle.dump(scX, (MODEL_DIR/f'narx/scaler_X_{cid}.pkl').open('wb'))\n",
    "    pickle.dump(scY, (MODEL_DIR/f'narx/scaler_Y_{cid}.pkl').open('wb'))\n",
    "\n",
    "    # train/val split 80/20\n",
    "    split = int(0.8 * len(Xc))\n",
    "    Xtr, Ytr = Xc[:split], Yc[:split]\n",
    "    Xvl, Yvl = Xc[split:], Yc[split:]\n",
    "\n",
    "    model = build_narx(Xc.shape[1], Yc.shape[1])\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss=weighted_mse)\n",
    "\n",
    "    es = callbacks.EarlyStopping(patience=8, restore_best_weights=True)\n",
    "    ck = callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_DIR/f'narx/cluster_{cid}',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_format='tf'\n",
    "    )\n",
    "\n",
    "    model.fit(scX.transform(Xtr), scY.transform(Ytr),\n",
    "              validation_data=(scX.transform(Xvl), scY.transform(Yvl)),\n",
    "              epochs=EPOCHS_NARX,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              verbose=0,\n",
    "              callbacks=[es, ck])\n",
    "\n",
    "print(\" NARX training done.\")\n",
    "\n",
    "# -- store global metadata (helps inference script remain agnostic) --------\n",
    "json.dump({'state_cols': STATE_COLS, 'exog_cols': EXOG_COLS, 'lag': LAG},\n",
    "          (MODEL_DIR/'metadata.json').open('w'))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 4. Collect NARX validation residuals across *all* clusters  ➜  training\n",
    "#    set for Quantile-Regression nets.\n",
    "# --------------------------------------------------------------------------\n",
    "val_X, val_E = [], []\n",
    "for cid in range(N_CLUSTERS):\n",
    "    scaler_x = pickle.loads((MODEL_DIR/f'narx/scaler_X_{cid}.pkl').read_bytes())\n",
    "    scaler_y = pickle.loads((MODEL_DIR/f'narx/scaler_Y_{cid}.pkl').read_bytes())\n",
    "    narx     = tf.keras.models.load_model(MODEL_DIR/f'narx/cluster_{cid}',\n",
    "                                          compile=False)\n",
    "\n",
    "    for p in train_files:\n",
    "        # predict cluster id directly from already-computed feature → faster\n",
    "        sig = sc_feat.transform([feat[kmeans.labels_ == cid][0]])  # re-use stats\n",
    "        if kmeans.predict(sig)[0] != cid:\n",
    "            continue\n",
    "\n",
    "        X, Y = make_xy(preprocess(p))\n",
    "        if not len(X):\n",
    "            continue\n",
    "\n",
    "        y_hat = scaler_y.inverse_transform(\n",
    "                    narx.predict(scaler_x.transform(X), verbose=0))\n",
    "        val_X.append(scaler_x.transform(X))\n",
    "        val_E.append(Y - y_hat)\n",
    "\n",
    "val_X = np.vstack(val_X)\n",
    "val_E = np.vstack(val_E)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 5. Train Quantile-Regression nets (τ = 0.1 & 0.9) on residuals.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print(\"\\n Training QR nets …\")\n",
    "\n",
    "MAX_QR_SAMPLES=40000\n",
    "val_X_qr=val_X[:MAX_QR_SAMPLES]\n",
    "val_E_qr=val_E[:MAX_QR_SAMPLES]\n",
    "\n",
    "loss_dir = MODEL_DIR / 'qr/loss_curves'\n",
    "loss_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for j, col in enumerate(STATE_COLS):\n",
    "    y_err = val_E_qr[:, j:j+1]      # residual for that state\n",
    "    for q in QUANTILES:\n",
    "        qr = build_qr(val_X_qr.shape[1])\n",
    "\n",
    "        \n",
    "        qr.compile(optimizer=optimizers.Adam(1e-4), loss=pinball_loss(q))\n",
    "\n",
    "        es_qr = callbacks.EarlyStopping(\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            monitor=\"val_loss\"\n",
    "        )\n",
    "\n",
    "        history = qr.fit(val_X_qr, y_err,\n",
    "                         validation_split=0.2,\n",
    "                         epochs=EPOCHS_QR,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         verbose=0,\n",
    "                         callbacks=[es_qr])\n",
    "\n",
    "        # Save model\n",
    "        qr.save(MODEL_DIR / f'qr/{col}_{q:.1f}')\n",
    "\n",
    "        # Plot loss curves\n",
    "        plt.figure()\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='val')\n",
    "        plt.title(f\"{col}  τ={q:.1f}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Pinball loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(loss_dir / f\"{col}_{q:.1f}_loss.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "print(\" QR nets done.\")\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 6. Conformal deltas  (90 % Prediction-Interval)  in **metres**.\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n Computing conformal deltas …\")\n",
    "alpha  = 0.1                     # 90 % coverage target\n",
    "deltas = {}\n",
    "for j, col in enumerate(STATE_COLS):\n",
    "    nonconf = np.abs(val_E[:, j])          # |error|\n",
    "    k       = int(np.ceil((1-alpha) * (len(nonconf)+1))) - 1\n",
    "    deltas[col] = float(np.sort(nonconf)[k])\n",
    "\n",
    "pickle.dump(deltas, (MODEL_DIR/'conformal_deltas.pkl').open('wb'))\n",
    "print(\" Saved deltas:\", deltas)\n",
    "\n",
    "print(\"\\n Finished training. Model at:\", MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
